# 摘要

作为情感分析的一项重要任务，面向目标的情感分类 (Targetoriented Sentiment Classification，**TSC**) 旨在识别句子中每个意见目标的情感极性。然而，这项任务的现有方法主要依赖于文本内容，而忽略了其他模态的数据(如图像)，而其他模态的数据可以增强这些基于文本的模型的鲁棒性。受这一观察的启发并受到最近提出的 BERT 架构的启发，我们研究了面向目标的多模态情感分类 (Target-oriented Multimodal Sentiment Classification，**TMSC**)，并提出了一个多模态 BERT 架构。为了对模态内动态进行建模，我们首先应用 BERT 来获得目标敏感的文本表示。然后，我们借鉴了自注意力机制的思想，设计了一种目标注意力机制来执行目标图像匹配，以得出目标敏感的视觉表示。为了对模态间动态进行建模，我们进一步建议在顶部堆叠一组自注意力层以捕获多模态交互。实验结果表明，我们的模型可以胜过 TSC 和 TMSC的几种极具竞争力的方法。

# 1. 引言
面向目标的情感分类 (TSC) 是情感分析的一项基本任务，旨在检测句子中提及的单个观点目标的情感方向。例如，给定一条推文“*Georgina Hermitage is a #one2watch since she broke the 400m T37 WR.*”，用户分别表达了对 *Georgina Hermitage* 和 *400m T37* 的**正面**和**负面**情绪。

为了解决这个问题，已经提出了各种具有浅层和深层文本特征的监督学习技术。随着注意力机制的最新趋势，许多研究提出了不同的基于注意力的神经架构来模拟意见目标与其周围上下文词之间的交互，可以进一步提高几个基准数据集上的最先进性能。

然而，上述所有方法都存在两个共同的**局限性**。
1. 它们中的大多数只随机初始化它们的模型参数，这可以通过仅使用一个小的、特定于任务的语料库优化它们而导致次优解决方案。随着最近在各种 NLP 任务中使用无监督语言模型预训练模型参数的趋势，很自然地期望这些经过良好初始化的模型可以捕获每个单词在不同上下文中的语义和句法含义，并为 TSC 带来更好的解决方案。
2. 这些现有方法主要依赖于文本内容，并且没有考虑其他相关的数据源（例如图像），这可能会补充文本内容并增强这些基于文本的模型。由于 Web 上的用户生成内容（例如推文、评论）越来越多模态，我们观察到相关图像对于 TSC 通常很有用，原因有几个。
    - 用户生成的句子通常集中在一个意见目标上，相关图像倾向于突出焦点目标（例如，图 1.a 中的 Georgina Hermitage 和图 1.b 中的 Joe Arpaio）。
    - 由于句子的简短和非正式的自然，有时很难检测到焦点目标的情绪，但相关的图像可能有助于反映用户对焦点目标的情绪（例如，在图 1 中，第 20 位国际人工智能联合会议 (IJCAI-19)5408 名用户的建议分别发布了 Georgina Hermitage 的愉快图像和 Joe Arpaio 的不愉快图像）。
    - 对于那些剩余的目标，句子通常表达对它们的中性情绪，图像也倾向于较少甚至不关注它们（例如，图 1 中的 400m T37 和亚利桑那州）。因此，探索如何构建意见目标和文本/视觉内容之间的对齐来对模态内动态进行建模会很有趣，然后融合文本和视觉表示以在统一的模型中揭示它们的模态间对齐面向目标的多模态情感分类 (TMSC)。

<img src="https://mayrain.space//upload/paper/image.png" alt="image.png" style="zoom: 67%;" />
为了解决这两个限制，在本文中，我们在最近的 BERT 架构之上构建了我们的模型，其来自大型语料库的预训练模型参数可以帮助获得上下文化的词表示，其 Transformer 编码器中使用的**多头自注意力机制**可以自动学习任意两个复杂对象之间的不同级别的对齐。

- 具体来说，我们首先将每个输入句子转换为两个子句：单个意见目标词和剩余的上下文词，并使用 BERT 来获得目标敏感的文本表示。
- 此外，受自注意力机制思想的启发，我们进一步设计了一种**目标注意机制**来自动学习**意见目标和图像之间的对齐**，其中目标被用作查询来监督模型，为相关图像中的不同区域分配适当的注意权重，以诱导目标敏感的视觉表示。
- 在对模态内对齐进行建模后，我们进一步在它们之上堆叠一组自注意力层，以自动捕获它们的**模态间交互**。

我们将此架构简称为面向目标的多模态 BERT 或 TomBERT。对TSC的三个基准数据集和两个TMSC手动注释的Twitter数据集的评估表明:首先，经过微调的BERT模型在TSC的三个基准数据集上的性能优于之前报告的最佳结果。其次，TomBERT 在 TMSC 的 Twitter 数据集上可以胜过最先进的基于文本的方法和极具竞争力的多模态方法。第三，进一步的分析表明，由于目标敏感的性质，当输入句子有多个意见目标时，TomBERT 特别有用。

我们的主要贡献总结如下：
- 我们为 TMSC 设计了一个面向目标的多模态 BERT 架构，其中底部的两个基于 BERT 的模块用于捕获模态内动态，包括目标-文本和目标图像对齐，并在顶部堆叠另一个基于 BERT 的模块以捕获模态间动态，即文本图像对齐。
- 我们建议使用标准 BERT 层对目标-文本和文本-图像对齐进行建模，并设计一个特殊的目标-图像匹配层与目标注意机制相结合来建模目标-图像对齐。

# 2. 相关工作
## 2.1 面向目标的情感分类（TSC）
作为情感分析的一项重要任务，面向目标的情感分类 (TSC) 近年来得到了广泛的研究 。一项工作侧重于利用外部资源手动设计一组特定于任务的特征，然后将传统的统计学习方法应用于情绪预测的特征。另一项工作集中在将目标信息合并到各种神经网络 (NN) 模型中，包括基于 Recusive NN 的方法、基于 RNN 的方法和基于CNN的方法。受注意力机制在其他 NLP 任务中的优势的启发，最近的许多研究设计了不同的基于注意力的方法来模拟目标和上下文之间的交互。然而，这些研究没有考虑可能促进这些基于文本的方法的视觉特征，这是本文的重点。最近，Xu 等人。通过提出一种多跳记忆网络来模拟跨模态和单模态交互来探索方面级多模态情感分析的任务。与他们的工作不同，我们旨在在本文中探索最近的 BERT 模型对 TSC 和 TMSC 的有用性。
## 2.2 多模态情感分类（MSC）
随着网络上多模态数据的增长，来自不同模态的信息（视觉、声学等）最近已被用于为传统的文本特征提供互补的情感信号。该领域的大多数现有研究都集中在对话中的情感分类上。具体来说，Poria等人(2015)和Poria等人(2017)分别提出了一种多核学习方法和基于lstm的顺序架构来融合文本特征、视觉特征和音频特征。在这一系列工作之后，Zadeh等人(2017)和Zadehet等人(2018)进一步设计了一个张量融合网络和一个记忆融合网络，以更好地捕获不同模态之间的相互作用。然而，这些方法是为粗粒度对话情感分类而设计的，这对于我们的**细粒度**面向目标的情感分类可能不是很有效。

# 3. 方法
文本提出了两个对BERT的多模态扩展：mBERT和TomBERT。
## 3.1 任务定义
我们给出了一组多模态样本 $D$。对于每个样本 $c\in D$，它包含一个带有 $n$ 个单词的句子$S$ $(w_1,\ldots,w_n)$ 和相关的图像 $I$，以及意见目标 $T$（$S$ 中单词的子序列）。对于意见目标 $T$ ，它也与情感标签 $y$ 相关联，可以是正面的、负面的或中性的。我们的问题可以表述如下：给定 $D$ 作为训练语料库，我们的目标是学习面向目标的情感分类器，以便它可以正确预测未见样本中意见目标的情感标签。

## 3.2 背景
获取目标敏感的文本特征
为了在TSC中采用BERT，将每个句子$S$转换成两个子句子：观点目标$T$和剩余的上下文$C$ ，并将他们串联起来作为BERT的输入序列。例如下表所示：
![image-rtmc.png](https://mayrain.space//upload/paper/image-rtmc.png)
为了将 BERT 用于 TSC，我们建议将每个句子 $S$ 转换为两个子句子：意见目标 $T$ 和剩余的上下文 $C$，并将它们连接为 BERT 的输入序列。例如，图1.a的BERT输入如表1所示。形式上，让我们使用$\mathbf{X}=(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_N)$ 表示转换后的输入序列，其中 $\mathbf{x}_i\in\mathbb{R}^d$ 是输入表示，通过对单词、段和位置嵌入求和，$N$ 是序列的最大长度。

接下来，让我们简要回顾一下 BERT 模型 ，它本质上是一个多层双向 Transformer 编码器，如图 2.a 的句子编码器部分所示。为了捕获全局信息，首先使用 m-head self-attention 层将输入序列中的每个位置转换为输入层的加权和。具体来说，对于第 $i$ 个头部注意力，输入层 $\mathbf{X}\in\mathbb{R}^{d\times N}$ 基于点积注意力机制进行变换，如下所示：

$$\mathrm{ATT}_i(\mathbf{X})=\mathrm{softmax}(\frac{[\mathbf{W}_{\mathbf{Q}_i}\mathbf{X}]^\top[\mathbf{W}_{\mathbf{K}_i}\mathbf{X}]}{\sqrt{d/m}})[\mathbf{W}_{\mathbf{V}_i}\mathbf{X}]^\top $$

其中 $\{\mathbf{W}_{\mathbf{Q}_i},\mathbf{W}_{\mathbf{K}_i},\mathbf{W}_{\mathbf{V}_i}\}\in\mathbb{R}^{d/m\times d}$分别是对应于查询、键和值的可学习参数。然后，将 $m$ 个注意力机制的输出连接在一起，然后进行线性变换，如下所示：

$$\text{МАТТ}(\mathbf{X})=\mathbf{W}_m[\text{АТТ}_1(\mathbf{X}),\ldots,\text{АТТ}_m(\mathbf{X})]^\top $$

其中 $\mathbf{W}_m\in\mathbb{R}^{d\times d}$ 是学习的参数。

基于自注意力层的输出，BERT 从输入到输出添加了一个残差连接，然后是一个层范数 (LN)，如下所示：

$$\mathbf{Z}=\mathrm{LN}(\mathbf{X}+\mathrm{MATT}(\mathbf{X}))$$

此外，以GeLU为激活函数的标准前馈网络(又称MLP)，顶部堆叠另一个与层范数的残差连接，生成第一个BERT层的输出:

$$\operatorname{BT}(\mathbf{X})=\operatorname{LN}(\mathbf{X}+\operatorname{MLP}(\mathbf{Z}))$$

最后，整个模型堆叠 $L_s$ 这样的 BERT 层，第一个标记的最终隐藏状态（即 [CLS]）被馈送到线性变换函数进行分类。

## 3.3 多模态BERT（mBERT）
mBERT模型提出的将图像纳入BERT架构的一个解决方案是直接将图像特征与获取目标敏感的文本特征连接起来，然后再上面堆叠额外的BERT层来模拟视觉和文本之间的模态间互动。下图为mBERT 的框架图：
<img src="https://mayrain.space//upload/paper/image-dycq.png" alt="image-dycq.png" style="zoom:67%;" />
具体来说，对于相关的图像 $I$，我们首先将其调整为 224×224 像素，然后采用最先进的图像识别模型 ResNet-152 (res5c) 来获得最后一个卷积层的输出：

$$\mathbf{ResNet}(\mathbf{I})=\{\mathbf{r}_j|\mathbf{r}_j\in\mathbb{R}^{2048},j=1,2,...,49\}$$

它本质上将原始图像分割为7 × 7 = 49个区域，每个区域由一个2048维向量$r_j$表示。接下来，我们使用线性变换函数将视觉特征投影到相同的文本特征空间：$\mathbf{G}=\mathbf{W}_v\text{ResNet}(\mathbf{I})$，其中 $\mathbf{W}_v\in\mathbb{R}^{d\times2048}$ 是可学习的参数。

此外，图像特征 $\mathbf{G}\in\mathbb{R}^{d\times49}$ 和文本特征 $\mathbf{H}_s\in\mathbb{R}^{d\times N}$（即最后一层的隐藏状态$\mathbf{BT}_{L_{s}}(\mathbf{X})$）连接在一起，然后将它们馈送到多模态编码器，该编码器包含另一组 BERT 层来自动建模文本和视觉特征之间的丰富交互：

$$\mathsf{ME}(\mathbf{G},\mathbf{H_S})=\mathsf{BT}_{L_m}([\mathbf{G},\mathbf{H_S}])$$

其中 $L_m$ 是多模态编码器中层数。最后，“[CLS]”标记的最终隐藏状态进行分类。

## 3.4 面向目标的多模态BERT（TomBERT）
尽管上述 mBERT 模型有望很好地捕捉模态间动态，但它的主要局限性在于其视觉表示对意见目标不敏感，因为无论考虑的目标如何，同一输入句子的视觉特征总是相同的。直观地说，以特定的意见目标为输入，通常只有相关图像的某些区域与其密切相关，其他区域应该被忽略以消除噪声。例如，在图 1.a 中，以“Georgina Hermitage”为目标，我们主要关注她的笑脸，而忽略其他背景。相比之下，以“400m T37”为目标，我们应该只关注包含“Women's 400m T37”的区域。在这种情况下，如果我们的模型考虑了整个图像并错误地推进了 20 位国际人工智能联合会议 (IJCAI-19)5410 关注笑脸，模型很可能做出错误的预测。受此启发，我们设计了一个目标图像 (TI) 匹配层，它使用 m-head 目标注意力机制在目标和图像之间执行匹配以获得目标敏感的视觉表示。
<img src="https://mayrain.space/upload/paper/image-jhhc.png" alt="image-jhhc.png" style="zoom:67%;" />
首先应用另一个 BERT 编码器来获得意见目标的隐藏表示：

$$\mathbf{H_T}=\operatorname{BT}_{L_c}(\mathbf{T})$$

其中 $\mathbf{T}\in\mathbb{R}^{d\times M}$ 和 $M$ 长度，$L$ 是层数。接下来，我们将目标HT的隐藏状态作为查询，区域图像特征气体键和值，从而利用目标引导模型将其与适当的区域对齐，即只对与目标密切相关的图像区域分配高注意力权重。具体来说，第 $i$ 个头部目标注意力采用以下形式：

$$\mathrm{ATT}_i(\mathbf{G},\mathbf{H}_\mathbf{T})=\mathrm{softmax}(\frac{[\mathbf{W}_{\mathbf{Q}_i^{\prime}H_T}]^\top[\mathbf{W}_{\mathbf{K}_i^{\prime}G}]}{\sqrt{d/m}})[\mathbf{W}_{\mathbf{V}_i^{\prime}G}]^\top $$

其中$\{\mathbf{W}_{\mathbf{Q}_i}',\mathbf{W}_{\mathbf{K}_i}',\mathbf{W}_{\mathbf{V}_i}'\}\in\mathbb{R}^{d/m\times d}$ 是参数。此外，与 BERT 类似，我们采用前馈网络和两层具有残差连接的范数来获得目标敏感的视觉输出：

$$\mathrm{TI(G,T)=LN(H_T+MLP(LN(H_T+MATT(G,H_T))))}$$

然后我们堆叠 $L_t$ 这样的 $\text{TI}$ 匹配层以获得最终的视觉表示：

$\mathbf{H_V}=\mathrm{TI}_{L_t}(\mathbf{G},\mathbf{T})$，

其中$\mathbf{H_V}\in\mathbb{R}^{d\times M}$和 $\mathbf{H_V}$ 中的每个隐藏状态本质上是相关图像中 49 个区域的加权和。

接下来，为了形成多模态输入表示，我们考虑两种连接类型如下：

- **全文本**：直接连接 $\mathbf{H_V}$ 和 $\mathbf{H_S}$；
- **First-Text**：只考虑$\mathbf{H_V}$中第一个元素（即目标输入中的特殊 [CLS] 令牌）的最终状态，并将 $\mathbf{H_V^0}$ 与 $\mathbf{H_S}$ 连接。

与 mBERT 类似，我们在顶部进一步添加了一个多模态编码器以获得最终的多模态隐藏表示：

$$\mathbf{H=ME(H_V,H_S)}\quad\mathrm{or}\quad\mathbf{H=ME(H_V^0,H_S)}$$

为了整合视觉和文本表示进行最终分类，
我们考虑以下三种池化类型来获得最终输出：

- FIRST：多模态输入序列的第一个标记始终是 49 个区域图像特征的加权和。它的最终隐藏状态被视为以视觉表示作为查询的聚合多模态序列表示，因此可以作为输出：$\mathbf{O}=\mathbf{H}^{0}$；
- CLS：类似地，特殊标记的最终隐藏状态（即句子输入中 [CLS] 标记）是具有文本表示的聚合表示作为查询，也可以用作输出：$\mathbf{O}=\mathbf{H}^{\mathrm{[CLS]}}$
- BOTH：我们将这两个隐藏状态连接起来作为混合输出：$\mathbf{O}=[\mathbf{H}^0,\mathbf{H}^{[\mathbf{CLS}]}]$。

最后，我们将$\mathbf{O}$馈送到线性函数，然后是用于面向目标的情感分类的 softmax 函数：

$$p(y|\mathbf{O})=\text{softmax}(\mathbf{W}^\top\mathbf{O})$$

其中 $\mathbf{W}\in\mathbb{R}^{(2)d\times3}$ 是可学习的参数。
为了优化 TomBERT 模型中的所有参数，目标是最小化标准交叉熵损失函数，如下所示：

$$\begin{aligned}\mathcal{J}&=-\frac{1}{|D|}\sum_{j=1}^{|D|}\log p(y^{(j)}|\mathbf{O}^{(j)})\end{aligned}$$


# 4. 实验
在本节中，我们进行了广泛的实验来回答以下研究问题： 
- RQ1：微调后的 BERT 模型在现有基准数据集和我们的两个数据集上都优于最先进的基于文本的方法。（第 4.2 节）
- RQ2：相关图像是否通常对 TSC 有用。我们的 TomBERT 模型能否对 BERT 带来显着的改进，并在我们的两个多模态数据集上取得了最佳性能。（第 4.2 节）
- RQ3：TomBERT 在池化层、多模态连接层和隐藏层数量方面的有效性是什么。（第 4.3 节）
- RQ4：TomBERT 相对于其他高度竞争的方法的关键优势是什么。(第 4.4 节)

## 4.1 实验设置
为了评估 BERT 和 TomBERT 的效果，我们采用了三个 TSC 基准数据集（即 SemEval-2014 Task 4 [Pontiki et al., 2014] 的 LAPTOP 和 REST，以及 [Dong et al., 2014] 构建的 TWITTER-14）和两个 TMSC 的多模态数据集（即 [Zhang et al., 2018b] 和 [Lu et al., 2018] 分别收集的 TWITTER-15 和 TWITTER-17）。LAPTOP 和 REST 分别由笔记本电脑和餐厅领域的亚马逊客户评论组成，三个 TWITTER 数据集分别包含 2010-2014、2014-2015 和 2016-2017 年发布的用户推文。由于两个公开可用的多模态数据集 TWITTER-15 和 TWITTER-17 仅提供每条推文中的注释目标（即实体），我们要求三个领域专家对每个目标的情绪进行注释，并将三个注释器中的多数标签作为黄金标签。对于空间限制，我们仅在表 2 中展示了 TWITTER-15 和 TWITTER-17 的基本统计数据。
我们在 [Devlin et al., 2018] 发布的预训练 uncased BERTbase 模型之上构建我们的 TomBERT 模型，并在每个数据集开发集上调整超参数。具体来说，对于基于 BERT 的模型，我们将学习率设置为 5e-5，注意力头数设置为 $m$ = 12，辍学率设置为 0.1。 TSC 和 TMSC 的所有模型的批量大小分别设置为 16 和 32。此外，对于 TomBERT，句子输入和目标输入的最大长度分别设置为 $N$ = 64 和 $M$ = 16。编码句子输入和目标输入的层数都设置为 12，即 $L_s$ = $L_c$ = 12，其中参数都是从预训练的 BERTbase 模型初始化的。所有模型都经过 8 个 epoch 的微调，并基于 PyTorch 使用 NVIDIA Tesla V100 GPU 实现。

![image-aegf.png](https://mayrain.space/upload/paper/image-aegf.png)

## 4.2 实验结果
**微调 BERT (RQ1) 的性能**

为了证明微调 BERT 对 TSC 的影响，我们首先将其与许多代表性方法进行比较：1）SVM，包括许多精心设计的语言特征； 2）AE-LSTM，结合方面嵌入和特定于目标的注意力机制； 3）TD-LSTM ，使用两个 LSTM 分别对目标的左侧上下文和右侧上下文进行建模； 4）。IAN ，提出了一种交互式注意机制来模拟目标和上下文之间的交互； 5）MemNet ，在词嵌入之上应用多跳注意机制，并以目标作为查询的位置嵌入； 6）RAM ，通过在从多跳注意力机制获得的表示之上应用 GRU 模型来构建神经架构； 7）TNet ，将 CNN 与特定于目标的转换进行调整以整合目标和上下文； 8）MGAN ，建立了一个多粒度注意力网络来融合目标和上下文。
我们在表 3 中的所有五个数据集和表 4 的文本模态部分报告了基于文本的方法的准确度 (ACC) 和 Macro-F1 分数。很容易发现 BERT 始终优于所有基线，这支持了我们的第一个动机，即预训练模型可以导致更好的最优解，从而为 TSC 带来改进。

<img src="https://mayrain.space/upload/paper/image-cfpo.png" alt="image-cfpo.png" style="zoom:67%;" />

**TomBERT (RQ2)的性能**

然后，我们考虑以下高度竞争的方法来评估我们的 TomBERT 模型：1）Res-Target：连接 HT 和 G 的最大池化； 2）BERT+BL，在 BERTbase 之上添加另一个 BERT 层，类似于 MBERT，但没有视觉特征。3)Res-MGAN，通过将 G 的最大池化与 MGAN 的隐藏表示连接起来，将文本和视觉内容的简单组合； 4）Res-MGAN-TFN，使用张量融合网络 (TFN) 来融合具有丰富交互的 Res-MGAN 中的文本和视觉表示； 5）ResBERT+BL 和 Res-BERT+BL-TFN，用 BERT+BL 替换 Res-MGAN 和 Res-MGAN-TFN 中的文本编码器； 6）mBERT（All-Text），第 3.3 节中详述的模型； 7）mPBERT 是 mBERT 的一种变体，它使用 G 的最大池化作为输入视觉特征，三种池化类型之一来获得最终输出； 8）TomBERT（All-Text），我们在第 3.4 节中详述的模型，我们使用 All-Text 连接和 CLS 池进行最终输出； 9）TomBERT (FIRST、CLS 或 BOTH)，我们的模型具有 First-Text 连接，以及最终输出的三种池化类型之一。

基于表 4，我们可以进行一些观察：
1）Res-Target 的性能相当有限，这意味着相关图像只对文本起到支持作用，不能针对面向目标的情感预测独立处理； 
2）由于模型参数的容量更高，BERT+BL 比 BERT 带来了轻微的改进； 
3）Res-MGAN 和 Res-BERT+BL 通常可以提高 MGAN 和 BERT+BL 的性能，这表明相关图像通常有助于提高基于文本的方法； 
4）有趣的是，尽管 TFN 学习了模态之间的丰富交互，但它甚至降低了 Res-MGAN 和 Res-BERT+BL 的性能。这表明复杂融合矩阵很难直接捕获两种模态之间的相互作用； 
5）mBERT 关联图像输入句子和预测标签关联和 mPBERT 在大多数情况下可以优于 BERT+BL，并且通常比 Res-BERT+BL-TFN 具有更好的性能，这表明顶部多模态编码器可以很好地捕获模态间交互； 
6）最后，无论我们使用的连接和池化类型如何，TomBERT 在 e 个两个数据集上都取得了最佳结果，并且这些增益中的大多数都是显着的，p<0.05。这些观察结果支持了我们的第二个动机，即 TomBERT 可以很好地捕获模态内和模态间动态。

<img src="https://mayrain.space/upload/paper/image-qccp.png" alt="image-qccp.png" style="zoom:67%;" />

## 4.3 TomBERT (RQ3)的进一步分析
为了回答 RQ3 中的问题，我们研究了 mBERT 和 TomBERT 中不同组件的影响。
首先，比较表 4 中的三种池化类型，我们观察到以下内容：
1）无论我们使用哪种池化类型，TomBERT 通常比 mPBERT 表现更好。由于它们之间的唯一区别是 TI Matching 模块，这表明我们的目标注意力机制能够生成目标敏感的视觉表示，这可能会导致显着的性能提升； 
2）对于 mPBERT，由于其视觉表示不是目标敏感的，因此使用其最终隐藏状态（即 FIRST）会导致性能有限是合理的。相比之下，使用 CLS 和 BOTH 可以学习更多地关注文本表示，并且结果要好得多； 
3）对于 TomBERT，由于视觉和文本表示都是目标敏感的，因此直观的是所有三种池化类型都可以带来有希望的结果。
此外，对于这两种连接类型，从表 4 可以看出，结合所有视觉特征（即 All-Text）通常比将所有视觉特征抽象为除 mPBERT (FIRST) 之外的所有向量会导致稍差的性能。这进一步证实了在 TMSC 中，图像用于支持文本进行目标情感检测，过多关注视觉特征可能会带来一些噪声并导致性能下降。
最后，由于 mPBERT 和 TomBERT 中的多模态编码器 (ME) 和 TI 匹配 (TIM) 模块可以堆叠多层，我们分析了它们的层数 $L_m$ 和 $L_t$ 的影响。如图 3 所示，对于 ME，当 $L_m$ = 4 时，mPBERT 和 TomBERT 可以达到最佳结果；而对于 TIM，当 $L_t$ 约为 5 时，TomBERT 表现最好。当进一步增加 $L_m$ 和 $L_t$ 时，结果变得更糟，这可能是由于模型参数的增加。

## 4.4 案例研究(RQ4)
为了更好地理解TomBERT的优势，我们进一步将单目标和多个目标的测试句子分为两类，并在表6中报告了它们的结果。很容易观察到，当输入句子有多个目标时，TomBERT的性能明显优于BERT+BL和mPBERT，这与我们的动机一致。
此外，我们选择了几个具有代表性的测试样本来比较不同方法的预测。在表 5 的左边，我们可以看到尽管 BERT+BL 和 mPBERT 错误地预测了 IKON 和 SG 的情绪。在图像的帮助下，我们的 TomBERT 模型可以识别推文的重点是 SG 以外的波段 IKON，因此预测聚焦目标 IKON 的情绪为正，其他目标 SG 为中性。表 5 右侧的目标 Henry Ford 和 Getty 图像也可以进行了类似的观察。

![image-iozs.png](https://mayrain.space/upload/paper/image-iozs.png)

# 结论
在本文中，我们研究了面向目标的多模态情感分类 (TMSC)，并提出了一种面向目标的多模态 BERT (TomBERT) 架构来有效地捕获模态内和模态间动态。对 TSC 和 TMSC 的五个数据集的广泛评估表明 BERT 和我们的 TomBERT 模型在检测单个意见目标的情感极性方面是有效的。